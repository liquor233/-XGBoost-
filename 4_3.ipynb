{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd0db956eb77c6256702a2b21b3c227e5d21cf60bdbf418ac52193d82189149c07c",
   "display_name": "Python 3.6.10 64-bit ('learn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "小麦数据集的下载方式：\n",
    "```\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       0      1       2      3      4      5      6  label\n",
       "0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220      0\n",
       "1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956      0\n",
       "2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825      0\n",
       "3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805      0\n",
       "4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175      0\n",
       "5  14.38  14.21  0.8951  5.386  3.312  2.462  4.956      0\n",
       "6  14.69  14.49  0.8799  5.563  3.259  3.586  5.219      0\n",
       "7  14.11  14.10  0.8911  5.420  3.302  2.700  5.000      0\n",
       "8  16.63  15.46  0.8747  6.053  3.465  2.040  5.877      0\n",
       "9  16.44  15.25  0.8880  5.884  3.505  1.969  5.533      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15.26</td>\n      <td>14.84</td>\n      <td>0.8710</td>\n      <td>5.763</td>\n      <td>3.312</td>\n      <td>2.221</td>\n      <td>5.220</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14.88</td>\n      <td>14.57</td>\n      <td>0.8811</td>\n      <td>5.554</td>\n      <td>3.333</td>\n      <td>1.018</td>\n      <td>4.956</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14.29</td>\n      <td>14.09</td>\n      <td>0.9050</td>\n      <td>5.291</td>\n      <td>3.337</td>\n      <td>2.699</td>\n      <td>4.825</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.84</td>\n      <td>13.94</td>\n      <td>0.8955</td>\n      <td>5.324</td>\n      <td>3.379</td>\n      <td>2.259</td>\n      <td>4.805</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.14</td>\n      <td>14.99</td>\n      <td>0.9034</td>\n      <td>5.658</td>\n      <td>3.562</td>\n      <td>1.355</td>\n      <td>5.175</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.38</td>\n      <td>14.21</td>\n      <td>0.8951</td>\n      <td>5.386</td>\n      <td>3.312</td>\n      <td>2.462</td>\n      <td>4.956</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14.69</td>\n      <td>14.49</td>\n      <td>0.8799</td>\n      <td>5.563</td>\n      <td>3.259</td>\n      <td>3.586</td>\n      <td>5.219</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>14.11</td>\n      <td>14.10</td>\n      <td>0.8911</td>\n      <td>5.420</td>\n      <td>3.302</td>\n      <td>2.700</td>\n      <td>5.000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>16.63</td>\n      <td>15.46</td>\n      <td>0.8747</td>\n      <td>6.053</td>\n      <td>3.465</td>\n      <td>2.040</td>\n      <td>5.877</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>16.44</td>\n      <td>15.25</td>\n      <td>0.8880</td>\n      <td>5.884</td>\n      <td>3.505</td>\n      <td>1.969</td>\n      <td>5.533</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    "import numpy as np \n",
    "data = pd.read_csv(\"data/seeds_dataset.txt\",header=None,sep=\"\\s+\",converters={7:lambda x:int(x)-1})\n",
    "\n",
    "data.rename(columns={7:\"label\"},inplace=True)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[23:05:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-mlogloss:0.97370\ttest-mlogloss:0.99220\n",
      "[1]\ttrain-mlogloss:0.86848\ttest-mlogloss:0.90933\n",
      "[2]\ttrain-mlogloss:0.77856\ttest-mlogloss:0.83772\n",
      "[3]\ttrain-mlogloss:0.70058\ttest-mlogloss:0.77274\n",
      "[4]\ttrain-mlogloss:0.63218\ttest-mlogloss:0.71444\n",
      "[5]\ttrain-mlogloss:0.57235\ttest-mlogloss:0.66438\n",
      "[6]\ttrain-mlogloss:0.51972\ttest-mlogloss:0.62166\n",
      "[7]\ttrain-mlogloss:0.47377\ttest-mlogloss:0.58758\n",
      "[8]\ttrain-mlogloss:0.43245\ttest-mlogloss:0.55461\n",
      "[9]\ttrain-mlogloss:0.39554\ttest-mlogloss:0.52758\n",
      "[10]\ttrain-mlogloss:0.36257\ttest-mlogloss:0.50333\n",
      "[11]\ttrain-mlogloss:0.33191\ttest-mlogloss:0.47824\n",
      "[12]\ttrain-mlogloss:0.30457\ttest-mlogloss:0.45627\n",
      "[13]\ttrain-mlogloss:0.28068\ttest-mlogloss:0.43823\n",
      "[14]\ttrain-mlogloss:0.25848\ttest-mlogloss:0.42292\n",
      "[15]\ttrain-mlogloss:0.23873\ttest-mlogloss:0.40888\n",
      "[16]\ttrain-mlogloss:0.22067\ttest-mlogloss:0.39536\n",
      "[17]\ttrain-mlogloss:0.20483\ttest-mlogloss:0.38378\n",
      "[18]\ttrain-mlogloss:0.19043\ttest-mlogloss:0.37348\n",
      "[19]\ttrain-mlogloss:0.17690\ttest-mlogloss:0.36382\n",
      "[20]\ttrain-mlogloss:0.16469\ttest-mlogloss:0.35582\n",
      "[21]\ttrain-mlogloss:0.15356\ttest-mlogloss:0.34785\n",
      "[22]\ttrain-mlogloss:0.14376\ttest-mlogloss:0.34116\n",
      "[23]\ttrain-mlogloss:0.13453\ttest-mlogloss:0.33556\n",
      "[24]\ttrain-mlogloss:0.12620\ttest-mlogloss:0.32958\n",
      "[25]\ttrain-mlogloss:0.11895\ttest-mlogloss:0.32479\n",
      "[26]\ttrain-mlogloss:0.11174\ttest-mlogloss:0.31887\n",
      "[27]\ttrain-mlogloss:0.10520\ttest-mlogloss:0.31377\n",
      "[28]\ttrain-mlogloss:0.09939\ttest-mlogloss:0.31127\n",
      "[29]\ttrain-mlogloss:0.09390\ttest-mlogloss:0.30725\n",
      "[30]\ttrain-mlogloss:0.08855\ttest-mlogloss:0.30388\n",
      "[31]\ttrain-mlogloss:0.08407\ttest-mlogloss:0.30158\n",
      "[32]\ttrain-mlogloss:0.08007\ttest-mlogloss:0.30070\n",
      "[33]\ttrain-mlogloss:0.07613\ttest-mlogloss:0.29822\n",
      "[34]\ttrain-mlogloss:0.07237\ttest-mlogloss:0.29668\n",
      "[35]\ttrain-mlogloss:0.06899\ttest-mlogloss:0.29506\n",
      "[36]\ttrain-mlogloss:0.06597\ttest-mlogloss:0.29561\n",
      "[37]\ttrain-mlogloss:0.06312\ttest-mlogloss:0.29403\n",
      "[38]\ttrain-mlogloss:0.06029\ttest-mlogloss:0.29362\n",
      "[39]\ttrain-mlogloss:0.05774\ttest-mlogloss:0.29258\n",
      "[40]\ttrain-mlogloss:0.05529\ttest-mlogloss:0.29187\n",
      "[41]\ttrain-mlogloss:0.05313\ttest-mlogloss:0.29198\n",
      "[42]\ttrain-mlogloss:0.05118\ttest-mlogloss:0.29115\n",
      "[43]\ttrain-mlogloss:0.04928\ttest-mlogloss:0.29071\n",
      "[44]\ttrain-mlogloss:0.04759\ttest-mlogloss:0.28985\n",
      "[45]\ttrain-mlogloss:0.04592\ttest-mlogloss:0.28915\n",
      "[46]\ttrain-mlogloss:0.04428\ttest-mlogloss:0.29032\n",
      "[47]\ttrain-mlogloss:0.04283\ttest-mlogloss:0.29113\n",
      "[48]\ttrain-mlogloss:0.04137\ttest-mlogloss:0.29119\n",
      "[49]\ttrain-mlogloss:0.04011\ttest-mlogloss:0.29144\n"
     ]
    }
   ],
   "source": [
    "mask = np.random.rand(len(data)) < 0.8\n",
    "train = data[mask]\n",
    "test = data[~mask]\n",
    "\n",
    "xgb_train = xgb.DMatrix(train.iloc[:,:6],label = train.label)\n",
    "xgb_test = xgb.DMatrix(test.iloc[:,:6],label=test.label)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "watchlist = [(xgb_train,'train'),(xgb_test,'test')]\n",
    "num_round= 50\n",
    "bst = xgb.train(params,xgb_train,num_round,watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "测试集错误率（softmax）：0.1\n"
     ]
    }
   ],
   "source": [
    "pred = bst.predict(xgb_test)\n",
    "error_rate = np.sum(pred!=test.label)/test.shape[0]\n",
    "print('测试集错误率（softmax）：{}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[23:05:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-mlogloss:0.97370\ttest-mlogloss:0.99220\n",
      "[1]\ttrain-mlogloss:0.86848\ttest-mlogloss:0.90933\n",
      "[2]\ttrain-mlogloss:0.77856\ttest-mlogloss:0.83772\n",
      "[3]\ttrain-mlogloss:0.70058\ttest-mlogloss:0.77274\n",
      "[4]\ttrain-mlogloss:0.63218\ttest-mlogloss:0.71444\n",
      "[5]\ttrain-mlogloss:0.57235\ttest-mlogloss:0.66438\n",
      "[6]\ttrain-mlogloss:0.51972\ttest-mlogloss:0.62166\n",
      "[7]\ttrain-mlogloss:0.47377\ttest-mlogloss:0.58758\n",
      "[8]\ttrain-mlogloss:0.43245\ttest-mlogloss:0.55461\n",
      "[9]\ttrain-mlogloss:0.39554\ttest-mlogloss:0.52758\n",
      "[10]\ttrain-mlogloss:0.36257\ttest-mlogloss:0.50333\n",
      "[11]\ttrain-mlogloss:0.33191\ttest-mlogloss:0.47824\n",
      "[12]\ttrain-mlogloss:0.30457\ttest-mlogloss:0.45627\n",
      "[13]\ttrain-mlogloss:0.28068\ttest-mlogloss:0.43823\n",
      "[14]\ttrain-mlogloss:0.25848\ttest-mlogloss:0.42292\n",
      "[15]\ttrain-mlogloss:0.23873\ttest-mlogloss:0.40888\n",
      "[16]\ttrain-mlogloss:0.22067\ttest-mlogloss:0.39536\n",
      "[17]\ttrain-mlogloss:0.20483\ttest-mlogloss:0.38378\n",
      "[18]\ttrain-mlogloss:0.19043\ttest-mlogloss:0.37348\n",
      "[19]\ttrain-mlogloss:0.17690\ttest-mlogloss:0.36382\n",
      "[20]\ttrain-mlogloss:0.16469\ttest-mlogloss:0.35582\n",
      "[21]\ttrain-mlogloss:0.15356\ttest-mlogloss:0.34785\n",
      "[22]\ttrain-mlogloss:0.14376\ttest-mlogloss:0.34116\n",
      "[23]\ttrain-mlogloss:0.13453\ttest-mlogloss:0.33556\n",
      "[24]\ttrain-mlogloss:0.12620\ttest-mlogloss:0.32958\n",
      "[25]\ttrain-mlogloss:0.11895\ttest-mlogloss:0.32479\n",
      "[26]\ttrain-mlogloss:0.11174\ttest-mlogloss:0.31887\n",
      "[27]\ttrain-mlogloss:0.10520\ttest-mlogloss:0.31377\n",
      "[28]\ttrain-mlogloss:0.09939\ttest-mlogloss:0.31127\n",
      "[29]\ttrain-mlogloss:0.09390\ttest-mlogloss:0.30725\n",
      "[30]\ttrain-mlogloss:0.08855\ttest-mlogloss:0.30388\n",
      "[31]\ttrain-mlogloss:0.08407\ttest-mlogloss:0.30158\n",
      "[32]\ttrain-mlogloss:0.08007\ttest-mlogloss:0.30070\n",
      "[33]\ttrain-mlogloss:0.07613\ttest-mlogloss:0.29822\n",
      "[34]\ttrain-mlogloss:0.07237\ttest-mlogloss:0.29668\n",
      "[35]\ttrain-mlogloss:0.06899\ttest-mlogloss:0.29506\n",
      "[36]\ttrain-mlogloss:0.06597\ttest-mlogloss:0.29561\n",
      "[37]\ttrain-mlogloss:0.06312\ttest-mlogloss:0.29403\n",
      "[38]\ttrain-mlogloss:0.06029\ttest-mlogloss:0.29362\n",
      "[39]\ttrain-mlogloss:0.05774\ttest-mlogloss:0.29258\n",
      "[40]\ttrain-mlogloss:0.05529\ttest-mlogloss:0.29187\n",
      "[41]\ttrain-mlogloss:0.05313\ttest-mlogloss:0.29198\n",
      "[42]\ttrain-mlogloss:0.05118\ttest-mlogloss:0.29115\n",
      "[43]\ttrain-mlogloss:0.04928\ttest-mlogloss:0.29071\n",
      "[44]\ttrain-mlogloss:0.04759\ttest-mlogloss:0.28985\n",
      "[45]\ttrain-mlogloss:0.04592\ttest-mlogloss:0.28915\n",
      "[46]\ttrain-mlogloss:0.04428\ttest-mlogloss:0.29032\n",
      "[47]\ttrain-mlogloss:0.04283\ttest-mlogloss:0.29113\n",
      "[48]\ttrain-mlogloss:0.04137\ttest-mlogloss:0.29119\n",
      "[49]\ttrain-mlogloss:0.04011\ttest-mlogloss:0.29144\n",
      "测试集错误率（softprob）：0.1\n"
     ]
    }
   ],
   "source": [
    "params['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(params,xgb_train,num_round,watchlist)\n",
    "pred_prob = bst.predict(xgb_test)\n",
    "pred_label = np.argmax(pred_prob,axis=1)\n",
    "error_rate = np.sum(pred_label!=test.label)/test.shape[0]\n",
    "print('测试集错误率（softprob）：{}'.format(error_rate))"
   ]
  }
 ]
}